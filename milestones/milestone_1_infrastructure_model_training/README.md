# ğŸ“Œ Milestone 1: Project Infrastructure & Neural Network from Scratch

## ğŸ“– Overview

This milestone focuses on setting up the project infrastructure, installing essential tools, and implementing a **Deep Neural Network from scratch using NumPy**.  
The model is designed for **classification tasks**, with a primary focus on **MNIST digit recognition**.

No high-level deep learning frameworks (TensorFlow / PyTorch) are used, ensuring a strong understanding of **core neural network concepts** such as forward propagation, backpropagation, and gradient descent.

---

## ğŸ› ï¸ Tools & Technologies

- Python  
- NumPy â€“ Numerical computation  
- Pillow (PIL) â€“ Image preprocessing  
- Matplotlib â€“ Training visualization  

---

## ğŸ§  Deep Neural Network Architecture

The `DeepNeuralNetwork` class implements a **4-layer feedforward neural network**:

Input Layer (784)
â†“
Hidden Layer 1 (128 neurons)
â†“
Hidden Layer 2 (64 neurons)
â†“
Output Layer (10 classes)


### ğŸ”‘ Key Parameters

- `sizes` â€“ Defines neurons per layer (e.g., `[784, 128, 64, 10]`)
- `epochs` â€“ Number of full training passes
- `learning_rate` â€“ Controls weight update step size

Weights are initialized using **scaled random normal initialization** to reduce exploding and vanishing gradients.

---

## ğŸ” Core Components

### âš™ï¸ Activation Functions

**Sigmoid**
- Used in hidden layers
- Maps values between 0 and 1
- Includes derivative computation for backpropagation

**Softmax**
- Used in the output layer
- Converts raw scores into probability distributions

---

### ğŸ“‰ Loss Function

**Binary Cross-Entropy Loss**
- Computes prediction error
- Supports gradient calculation for backpropagation

---

## ğŸ”„ Forward & Backward Propagation

### â–¶ï¸ Forward Pass
- Computes weighted sums and activations layer by layer
- Stores intermediate values for backpropagation

### â—€ï¸ Backward Pass
- Implements backpropagation to compute gradients
- Uses the chain rule with activation and loss derivatives

### ğŸ”§ Parameter Update
- Uses **Stochastic Gradient Descent (SGD)**
- Weight update rule:
W = W - learning_rate Ã— gradient
---

## ğŸ“Š Training & Evaluation

- The `train()` method handles epoch-wise training
- Computes training and validation accuracy & loss
- Metrics are stored and plotted to visualize learning trends

---

## ğŸ–¼ï¸ Image Preprocessing

The `preprocess_image()` function performs the following steps:

- Loads image using Pillow
- Converts to grayscale
- Resizes to 28 Ã— 28
- Normalizes pixel values
- Flattens into a 784-length vector

---

## ğŸ” Digit Prediction

The `predict_digit()` function:

- Reshapes image input into column format
- Performs a forward pass
- Uses `argmax` to predict the digit (0â€“9)

---

## ğŸ“ˆ Experimental Observations (ReLU Activation)

### âš™ï¸ Configuration

- Learning Rate: 0.01
- Epochs: 100

### ğŸ“Š Results

- Training Accuracy: 99.90%
- Validation Accuracy: 97.45%
- Training Loss: 0.0117
- Validation Loss: 0.0825

### ğŸ§ª Analysis

ReLU activation significantly outperforms sigmoid in convergence speed and validation accuracy.  
A small trainâ€“validation gap indicates mild overfitting, but overall performance is strong, making **ReLU the most effective activation** tested for this architecture.

---

## âœ… Milestone Outcome

- Neural network successfully implemented from scratch
- End-to-end training and inference pipeline completed
- Strong validation performance achieved
- Solid foundation established for future milestones
