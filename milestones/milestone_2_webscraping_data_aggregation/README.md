Milestone 2: Webscrapping and data aggregation
This module focuses on two core data engineering and NLP tasks:

Question Answering (QA) using Transformer-based NLP models

Web Scraping and Data Aggregation from both dynamic and structured websites

The goal is to analyze QA model performance under varying questionâ€“context scenarios and to implement robust web scraping pipelines using industry-standard tools.

ğŸ¯ Objectives

ğŸ” Evaluate transformer-based extractive Question Answering models

ğŸŒ Scrape dynamic JavaScript-rendered websites using Playwright

ğŸ•· Crawl structured paginated websites using Scrapy

ğŸ“ Store extracted data in CSV and JSON formats

âš™ Address real-world environment challenges in Google Colab

ğŸ›  Tools & Technologies Used
ğŸ”  Programming & Environment

Python 3.x

Google Colab (Linux)

ğŸ§  NLP & Machine Learning

ğŸ¤— Hugging Face Transformers

deepset/roberta-base-squad2

deepset/tinyroberta-squad2

ğŸŒ Web Scraping

ğŸ­ Playwright (Dynamic & AJAX pages)

ğŸ•· Scrapy (Structured crawling)

ğŸ“¦ Libraries & Dependencies

asyncio, nest_asyncio

json, csv, pathlib

os, sys, subprocess, tempfile, site

Pandas (Data handling & visualization)

ğŸ§  NLP Question Answering Module
ğŸ“Œ Description

Uses pre-trained RoBERTa models fine-tuned on SQuAD 2.0

Performs extractive QA by identifying answer spans in a given context

âš™ Implementation Highlights

pipeline("question-answering") from Hugging Face

Inputs: Question + Context

Outputs: Answer + Confidence Score

ğŸ“ˆ Key Observations

âœ… High confidence for direct questions

âš  Low confidence for indirect or inference-based questions

ğŸš« Correctly returns no answer for unrelated questions

âš¡ TinyRoBERTa performs efficiently for straightforward queries

ğŸŒ Web Scraping Module
ğŸ­ Playwright (Dynamic Content)

Targets:

Laptops & Tablets â€“ webscraper.io

Features:

Handles AJAX & JavaScript rendering

Supports:

â€œLoad Moreâ€ buttons

Multi-page pagination

Data Extracted:

Product Title

Price

Rating

Product URL

Image URL

Output Formats: CSV & JSON

ğŸ•· Scrapy (Structured Crawling)

Target Website:

ğŸ“š books.toscrape.com

Features:

Recursive pagination

CSS selector-based extraction

Textual â†’ numeric rating conversion

Absolute URL normalization

Colab Challenge Solved Using:

Temporary execution script

Manual PYTHONPATH setup

scrapy.cmdline.execute() via subprocess

ğŸ“Š Data Preprocessing

âœ‚ Whitespace trimming

ğŸ”¢ Rating standardization

ğŸ”— Relative â†’ Absolute URL conversion

ğŸ“„ Direct serialization to CSV & JSON

No advanced normalization or currency conversion was applied to preserve raw extracted values.

ğŸ“ˆ Model Evaluation
ğŸ§  QA Model

Confidence score used as reliability indicator

Performs best with high context relevance

Robust handling of unanswerable queries

ğŸŒ Web Scraping

Validated through:

Product count verification

Pandas DataFrame inspection

CSV & JSON output integrity

ğŸ Conclusion

This project successfully demonstrates:

âœ… Practical application of Transformer-based QA models

âœ… Efficient scraping of dynamic and static websites

âœ… Proper tool selection based on website architecture

âœ… Real-world debugging in cloud-based environments

ğŸš€ Future Enhancements

ğŸ“Š Add Exact Match & F1-score evaluation for QA

ğŸ’± Normalize prices and textual attributes

ğŸ›¡ Handle CAPTCHAs and rate limiting

ğŸ”„ Integrate scraped data directly into QA pipelines
